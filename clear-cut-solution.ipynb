{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for reading, cleaning and plotting the dataa\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Libraries for models \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data \n",
    "training_data = []\n",
    "with open('data/train.csv', newline='') as csvfile:\n",
    "    train_data = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in train_data:\n",
    "        training_data.append(row)\n",
    "            \n",
    "# Convert to a numpy array of type int (except for the label row)\n",
    "training_data = np.array(training_data[1:]).astype(int)   \n",
    "\n",
    "# Read in test data\n",
    "testing_data = []\n",
    "with open('data/test.csv', newline='') as csvfile:\n",
    "    test_data = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in test_data:\n",
    "        testing_data.append(row)\n",
    "\n",
    "# The testing file is huge so only read in max_test_data\n",
    "max_test_data = 30001\n",
    "test_data = np.array(testing_data[1:max_test_data]).astype(int)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
    "# permutation to X and Y.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "shuffle = np.random.permutation(np.arange(training_data.shape[0]))\n",
    "training_data = training_data[shuffle]\n",
    "\n",
    "# Split training data (labeled) into 80% training and 20% dev) and skip over the id column (it doesn't add an information)\n",
    "# Immediately cast train data as floats so we can normalize it later \n",
    "split_index = int(len(training_data) * 0.8)\n",
    "train_data = training_data[:split_index, 1:-1].astype(np.float64)\n",
    "train_labels = training_data[:split_index, -1]\n",
    "dev_data = training_data[split_index:, 1:-1].astype(np.float64)\n",
    "dev_labels = training_data[split_index:, -1]\n",
    "test_data = test_data[:,1:]\n",
    "\n",
    "# Retrieve the mean and standard deviation of each feature - axis=0 is for going along the columns, keepdims forces the dimensions to stay the same\n",
    "# Only compute it for the first ten features (they're numeric - not one hot or categorical)\n",
    "num_columns = 10\n",
    "# To avoid dividing by 0, add 1e-10 to the standard deviation \n",
    "smoothing = 1e-10\n",
    "# USe the mean and standard deviation of the training data\n",
    "feature_mean = train_data[:,:num_columns].mean(axis=0, keepdims=True)\n",
    "feature_std = train_data[:,:num_columns].std(axis=0, keepdims=True)\n",
    "# Normalize all numeric features except wilderness type and soil type (one-hot) - first 10 columns\n",
    "train_data[:,:num_columns] = train_data[:,:num_columns] - feature_mean\n",
    "train_data[:,:num_columns] = np.divide(train_data[:,:num_columns], feature_std + smoothing)\n",
    "# Normalize dev data as well (using training mean and standard deviation)\n",
    "dev_data[:,:num_columns] = dev_data[:,:num_columns] - feature_mean\n",
    "dev_data[:,:num_columns] = dev_data[:,:num_columns]/(feature_std + smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (12096, 54) Training labels shape: (12096,)\n",
      "Dev data shape: (12096, 54) Dev labels shape: (12096,)\n",
      "Test data shape:  (30000, 54)\n",
      "First training example:  [-1.0874239   1.39067033  0.65424367 -0.36479144  0.2456598  -0.58855921\n",
      " -1.9018828   0.04564075  1.48886239 -1.12134094  0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ] 3\n",
      "First dev example:  [-0.78171192 -0.57407384 -0.88760796 -0.79554381 -0.99323992 -1.24549038\n",
      "  0.72960321  0.30928477 -0.30640172 -1.02432588  0.          0.\n",
      "  1.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ] 6\n",
      "First test example:  [2680  354   14    0    0 2684  196  214  156 6645    1    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    1    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Explore and confirm the shape of the data\n",
    "print(\"Training data shape: {0} Training labels shape: {1}\".format(train_data.shape, train_labels.shape))\n",
    "print(\"Dev data shape: {0} Dev labels shape: {1}\".format(train_data.shape, train_labels.shape))\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"First training example: \", train_data[0], train_labels [0])\n",
    "print(\"First dev example: \", dev_data[0,:], dev_labels [0])\n",
    "print(\"First test example: \", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance for 1 trees: 0.48214285714285715\n",
      "Random Forest Confusion Matrix:\n",
      "\n",
      "[[ 73  35   0   0  11   1  21]\n",
      " [167 184   1   0 129   2  82]\n",
      " [  0   2  94   7   2  63   0]\n",
      " [  1   1  30 358   2  35   0]\n",
      " [156 165 139   1 296 154  90]\n",
      " [  0   1 166  67   0 178   0]\n",
      " [ 34   1   0   0   0   0 275]]\n",
      "Random Forest Performance for 3 trees: 0.6967592592592593\n",
      "Random Forest Confusion Matrix:\n",
      "\n",
      "[[240  95   0   0   5   0  27]\n",
      " [ 86 186   6   0  51  11   1]\n",
      " [  1   7 218  31   6  51   0]\n",
      " [  1   0  34 380   5  30   0]\n",
      " [ 30  83  36   0 343  38   3]\n",
      " [  0  10 136  22  30 303   0]\n",
      " [ 73   8   0   0   0   0 437]]\n",
      "Random Forest Performance for 5 trees: 0.7013888888888888\n",
      "Random Forest Confusion Matrix:\n",
      "\n",
      "[[274 103   0   0   0   0  33]\n",
      " [ 55 156   2   0  22   2   6]\n",
      " [  0   1 250  65  11  99   0]\n",
      " [  0   0  18 340   0  23   0]\n",
      " [ 36 100  23   0 393  30   0]\n",
      " [  1  17 137  28  14 279   0]\n",
      " [ 65  12   0   0   0   0 429]]\n",
      "Random Forest Performance for 10 trees: 0.7351190476190477\n",
      "Random Forest Confusion Matrix:\n",
      "\n",
      "[[274  98   0   0   2   2  44]\n",
      " [ 73 189   5   0  33   2   3]\n",
      " [  0   4 270   9  15  61   0]\n",
      " [  0   0  58 413   0  41   0]\n",
      " [ 44  83   7   0 365  30   6]\n",
      " [  1  11  90  11  25 297   0]\n",
      " [ 39   4   0   0   0   0 415]]\n",
      "Random Forest Performance for 100 trees: 0.7589285714285714\n",
      "Random Forest Confusion Matrix:\n",
      "\n",
      "[[295 114   0   0   1   0  33]\n",
      " [ 50 178   3   0  31   1   0]\n",
      " [  0   8 296  15  14  76   0]\n",
      " [  0   0  41 413   0  35   0]\n",
      " [ 29  73  10   0 379  22   0]\n",
      " [  1  11  80   5  15 299   0]\n",
      " [ 56   5   0   0   0   0 435]]\n"
     ]
    }
   ],
   "source": [
    "# Try a random forest - before any data cleaning \n",
    "def RandomForest(num_trees):\n",
    "    model = RandomForestClassifier(num_trees,max_depth=8)\n",
    "    model.fit(train_data, train_labels)\n",
    "    predictions = model.predict(dev_data)\n",
    "    score = model.score(dev_data, dev_labels)\n",
    "    print(\"Random Forest Performance for {0} trees: {1}\".format(num_trees,score))\n",
    "    print(\"Random Forest Confusion Matrix:\\n\")\n",
    "    print(confusion_matrix(predictions, dev_labels))\n",
    "    \n",
    "num_trees_list = [1,3,5,10,100]\n",
    "for num_trees in num_trees_list:\n",
    "    RandomForest(num_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

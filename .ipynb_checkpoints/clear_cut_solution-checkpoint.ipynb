{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type Prediction\n",
    "#### Team: Clear-Cut Solution: Kevin Martin, Yang Jing, Justine Schabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Introduce the project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for reading, cleaning and plotting the dataa\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature Engineering was written by the team \n",
    "import feature_engineering as fe\n",
    "# Models was written by the team \n",
    "import models \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data \n",
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "The following transformations were made in the function below. \n",
    "\n",
    "#### Transform Hillshade\n",
    "Now we'll normalize the \"Hillsdale\" variables by dividing them by 255. The hillshade variables contain index of shades with a value between 0 and 255. \n",
    "\n",
    "#### Create new soil types\n",
    "Now we'll create additional features to magnify the differences betweeen cover type1 and 2, and covery type3 and 6.\n",
    "\n",
    "#### Combine soil types \n",
    "\n",
    "#### Drop rare or non-existant soil types \n",
    "Now we'll drop soil types that don't exist in the training set. Then we will combine soil types 35, 38, 39 and 40 because they have a very similar distribution. \n",
    "\n",
    "#### Create new features based on soil type descriptions \n",
    "TODO: Explain how we split up soil descriptions into different features to account for overlap.\n",
    "\n",
    "#### Transform Aspect\n",
    "TODO: Explain aspect problem and solution\n",
    "\n",
    "#### Log transformations\n",
    "TODO: Now we'll log transform the features related to the distances. (explain why)\n",
    "\n",
    "#### Add polynomial features\n",
    "TODO: Explain why we're making Elevation polynomial\n",
    "\n",
    "#### Drop irrelevant or problematic features\n",
    "- We'll drop \"Id\" because it does not provide any meaning in the classifications.\n",
    "- We'll drop \"Hillshade_9am\" because it has a high correlation with \"Aspect\" and \"Hillshade_3pm\".\n",
    "- TODO: We'll also drop \"Vertical_Distance_To_Hydrology\" because _.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scale_hillside' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f79c49e1cd7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanipulate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-f79c49e1cd7e>\u001b[0m in \u001b[0;36mmanipulate_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_examples\u001b[0m \u001b[0mx\u001b[0m \u001b[0mm_features\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_hillside\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Soil Combination One (based on distributions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scale_hillside' is not defined"
     ]
    }
   ],
   "source": [
    "def manipulate_data(data):\n",
    "    \"\"\" \n",
    "    This function applys transformations on the input data set\n",
    "    \n",
    "    Parameters: \n",
    "        data (dataframe): n_examples x m_features (int64) dataframe \n",
    "    \"\"\"\n",
    "    data = fe.scale_hillside(data)\n",
    "    \n",
    "    # Soil Combination One (based on distributions)\n",
    "    data = fe.combine_environment_features(data)\n",
    "    data = fe.drop_unseen_soil_types(data)  \n",
    "    data = fe.combine_soil_types(data)\n",
    "      \n",
    "    \n",
    "    # Soil Combination Two (based on descriptions)\n",
    "    # data = fe.set_soil_type_by_attributes(data)\n",
    "    \n",
    "    data = fe.transform_aspect(data)\n",
    "    \n",
    "    features_to_log = ['Horizontal_Distance_To_Hydrology',\n",
    "           'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']\n",
    "    data = fe.log_features(data, features_to_log)\n",
    "    \n",
    "    features_to_square = [\"Elevation\"]\n",
    "    data = fe.add_polynomial_features(data, features_to_square)\n",
    "\n",
    "    # These are already being dropped by now? \n",
    "    features_to_drop = [\"Id\",\"Hillshade_9am\",\"Vertical_Distance_To_Hydrology\"]\n",
    "    data = fe.drop_features(data, features_to_drop)\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_df = manipulate_data(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is transformed, we can also visualize the new aspect features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cover type VS the cosine of Aspect degerees\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(12,4))\n",
    "sns.violinplot(x=train_df['Cover_Type'],y=train_df['ap_ew'],ax=ax1)\n",
    "sns.histplot(train_df['ap_ew'],ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the feature transformation, we see improved distinction in median values, espeically for cover type 6, where the median is notably higher than that of other cover types and the distribution is concentrated around the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train/dev\n",
    "\n",
    "Then, we split the training data into a training data set (80%) and development data set (20%). We will also have a large, separate test data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, dev_data, dev_labels = fe.split_data(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data to have a mean of 0 and a variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_features = ['Elevation','Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Horizontal_Distance_To_Roadways',\n",
    "       'Horizontal_Distance_To_Fire_Points','Elevation_squared']\n",
    "train_data, train_scaler = fe.scale_training_data(standardize_features, train_data, scaler_type=\"standard\")\n",
    "dev_data = fe.scale_non_training_data(standardize_features, dev_data, train_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore and confirm the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape: {0} Training labels shape: {1}\\n\".format(train_data.shape, train_labels.shape))\n",
    "print(\"Dev data shape: {0} Dev labels shape: {1}\\n\".format(dev_data.shape, dev_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees_list = [1,3,5,10,100]\n",
    "random_forest_models = []\n",
    "random_forest_results = {}\n",
    "for num_trees in num_trees_list:\n",
    "    score, probabilities, random_forest_model = models.random_forest(num_trees, train_data, train_labels, dev_data, dev_labels)\n",
    "    random_forest_results[score] = probabilities\n",
    "    random_forest_models.append(random_forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_list = [1,2,4, 7, 10]\n",
    "knn_models = []\n",
    "knn_results = {}\n",
    "for neighbor in neighbor_list:\n",
    "    score, probabilities, knn_model = models.k_nearest_neighbors(neighbor,train_data, train_labels, dev_data, dev_labels)\n",
    "    knn_results[score] = probabilities\n",
    "    knn_models.append(knn_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_results = {}\n",
    "score, probabilities,mlp_model = models.multi_layer_perceptron(train_data, train_labels, dev_data, dev_labels)\n",
    "mlp_results[score] = probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.logistic_regression(train_data, train_labels, dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.neural_network(train_data, train_labels, dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble\n",
    "Here we will combine the three best performing models and implement a \"voting\" system to try to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes, new_predictions = models.ensemble(mlp_results,knn_results,random_forest_results, dev_labels)\n",
    "mse_ensemble = mean_squared_error(dev_labels, new_predictions)\n",
    "accuracy = accuracy_score(dev_labels, new_predictions)\n",
    "print(\"Mean Squared Error: \", mse_ensemble)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine and Compare Histograms of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2)\n",
    "# Ensemble\n",
    "axes[0,0].hist(new_predictions, bins=7,color = 'red') \n",
    "# MLP\n",
    "axes[0,1].hist(predicted_classes[:,0], bins=7, color = 'orange') \n",
    "# KNN\n",
    "axes[1,0].hist(predicted_classes[:,1], bins=7, color = 'green') \n",
    "# Random Forest\n",
    "axes[1,1].hist(predicted_classes[:,2], bins=7, color = 'blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results\n",
    "#### Read in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data \n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "# Preserve testing df ID for submission purpose\n",
    "test_df_ID = test_data[\"Id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the same transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = manipulate_data(test_data)\n",
    "test_data = fe.scale_non_training_data(standardize_features, test_data, train_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_forest_predictions = random_forest_models[-1].predict(test_data)\n",
    "# knn_predictions = knn_models[0].predict(test_data)\n",
    "# mlp_predictions = mlp_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_submission(y_pred,model):\n",
    "#     result = pd.DataFrame.from_dict(dict(zip(test_df_ID.to_list(),y_pred)), orient='index', columns=[\"Cover_Type\"])\n",
    "#     result.to_csv(f\"submissions/submission{model}.csv\",index_label=\"Id\")\n",
    "\n",
    "# gen_submission(random_forest_predictions, model=\"RandomForest\")\n",
    "# gen_submission(knn_predictions, model=\"KNN\")\n",
    "# gen_submission(mlp_predictions, model=\"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End matter\n",
    "\n",
    "#### Acknowledgements/Sources\n",
    "\n",
    "* That helpful stack overflow post\n",
    "  * https://stackoverflow.com/questions/28663856/how-to-count-the-occurrence-of-certain-item-in-an-ndarray\n",
    "* Relevant Documentation\n",
    "  * KNeighborsClassifier\n",
    "    * https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "  * Pretty Confusion Matrix\n",
    "    * https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
    "  * Preprocessing\n",
    "    * https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n",
    "* Soil information\n",
    "  * https://www.uidaho.edu/cals/soil-orders/aridisols\n",
    "  \n",
    "#### Backup Formats\n",
    "\n",
    "*because sometimes you just want to look at the markdown or whatever real quick*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a backup of the jupyter notebook in a format for where changes are easier to see.\n",
    "!jupyter nbconvert clear_cut_solution.ipynb --to=\"python\" --output=\"backups/clear_cut_solution\"\n",
    "!jupyter nbconvert clear_cut_solution.ipynb --to markdown --output=\"backups/clear_cut_solution\"\n",
    "\n",
    "# Also archiving this bad boy\n",
    "!jupyter nbconvert clear_cut_solution.ipynb --to html --output=\"backups/clear_cut_solution\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
